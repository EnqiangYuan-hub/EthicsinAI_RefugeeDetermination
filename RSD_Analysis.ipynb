{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# RSD AI Case Study \u2014 Data Visualization & Analysis\n\nThis notebook guides you through exploring the synthetic Refugee Status Determination (RSD) dataset to uncover patterns of bias in automated decision-making.\n\n**Structure:**\n- 0. Helper Functions\n- 1. Import & Preview Data\n- 2. Explore Applicant Demographics\n- 3. Examine Scoring Functions\n- 4. Analyze AI & Final Decisions\n- 5. Human Oversight & Override Patterns\n- 6. Appeals & Bias Audit\n- 7. Build a Predictive Model\n- 8. Discussion Questions"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 0. Helper Functions\n\nRun this cell first. It defines all the plotting utilities used throughout the notebook."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nsns.set_theme(style=\"whitegrid\", palette=\"muted\")\n\n\ndef viewDecisionDistribution(data, decision_col='final_decision'):\n    \"\"\"Pie chart of approve vs deny for any decision column.\"\"\"\n    counts = data[decision_col].value_counts()\n    colors = ['#66c2a5', '#fc8d62']\n    plt.figure()\n    plt.pie(counts, labels=counts.index, autopct='%1.1f%%',\n            startangle=90, colors=colors)\n    plt.title(f'Distribution of {decision_col}')\n    plt.tight_layout()\n    plt.show()\n\n\ndef viewApprovalRateBy(data, groupby_col, decision_col='final_decision'):\n    \"\"\"Bar chart showing approval rate broken down by any categorical variable.\"\"\"\n    approval_rate = (\n        data.groupby(groupby_col)[decision_col]\n        .apply(lambda x: (x == 'approve').mean())\n        .sort_values()\n        .reset_index()\n    )\n    approval_rate.columns = [groupby_col, 'approval_rate']\n    plt.figure(figsize=(8, 4))\n    plt.barh(approval_rate[groupby_col], approval_rate['approval_rate'],\n             color='#66c2a5', edgecolor='white')\n    plt.axvline(x=approval_rate['approval_rate'].mean(), color='#fc8d62',\n                linestyle='--', linewidth=1.5, label='Overall average')\n    plt.xlabel('Approval Rate')\n    plt.title(f'Final Decision Approval Rate by {groupby_col}')\n    plt.xlim(0, 1)\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\n\ndef viewScoreDistribution(data, score_col, hue_col=None):\n    \"\"\"Histogram of any continuous score, optionally split by a category.\"\"\"\n    plt.figure(figsize=(8, 4))\n    if hue_col:\n        for val in data[hue_col].unique():\n            subset = data[data[hue_col] == val]\n            sns.kdeplot(subset[score_col], label=str(val), fill=True, alpha=0.4)\n        plt.legend(title=hue_col)\n    else:\n        sns.histplot(data[score_col], bins=30, kde=True, color='#66c2a5')\n    plt.xlabel(score_col)\n    plt.ylabel('Density')\n    plt.title(f'Distribution of {score_col}' + (f' by {hue_col}' if hue_col else ''))\n    plt.tight_layout()\n    plt.show()\n\n\ndef viewScoreByDecision(data, score_col, decision_col='final_decision'):\n    \"\"\"Boxplot comparing a score across approve vs deny groups.\"\"\"\n    plt.figure(figsize=(6, 4))\n    sns.boxplot(data=data, x=decision_col, y=score_col,\n                palette={'approve': '#66c2a5', 'deny': '#fc8d62'})\n    plt.title(f'{score_col} by {decision_col}')\n    plt.tight_layout()\n    plt.show()\n\n\ndef viewCategoricalBreakdown(data, category_col, hue_col='final_decision'):\n    \"\"\"Grouped bar chart for any categorical variable split by a hue.\"\"\"\n    plt.figure(figsize=(9, 4))\n    sns.countplot(data=data, x=category_col, hue=hue_col,\n                  palette={'approve': '#66c2a5', 'deny': '#fc8d62',\n                           'Male': '#66c2a5', 'Female': '#fc8d62', 'Non-binary': '#8da0cb'},\n                  order=data[category_col].value_counts().index)\n    plt.xticks(rotation=45, ha='right')\n    plt.title(f'{category_col} breakdown by {hue_col}')\n    plt.tight_layout()\n    plt.show()\n\n\ndef plotCorrelationHeatmap(data, cols):\n    \"\"\"Heatmap of correlations between selected numeric columns.\"\"\"\n    plt.figure(figsize=(8, 6))\n    corr = data[cols].corr()\n    sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm',\n                center=0, linewidths=0.5)\n    plt.title('Correlation Matrix')\n    plt.tight_layout()\n    plt.show()\n\n\ndef plotFeatureImportance(model, X_train):\n    \"\"\"Bar chart of feature importances from a trained Random Forest.\"\"\"\n    importances = pd.Series(model.feature_importances_, index=X_train.columns)\n    importances = importances.sort_values(ascending=True)\n    plt.figure(figsize=(7, 5))\n    importances.plot(kind='barh', color='#66c2a5', edgecolor='white')\n    plt.xlabel('Feature Importance')\n    plt.title('Feature Importance from Random Forest')\n    plt.tight_layout()\n    plt.show()\n\nprint(\"Helper functions loaded!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Import & Preview Data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the dataset \u2014 update path if running locally\n# If using Google Colab, upload the CSV or mount your Drive first\ndf = pd.read_csv('../dataset/synthetic_RSD_dataset.csv')\n\nprint(f\"Dataset shape: {df.shape}\")\ndf.head(10)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Column types and null counts\ndf.info()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Summary statistics for numeric columns\ndf.describe().round(2)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Explore Applicant Demographics\n\nBefore looking at decisions, let's understand who is in this dataset."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Gender distribution\nviewDecisionDistribution(df, decision_col='gender')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Country of origin breakdown by gender\nviewCategoricalBreakdown(df, 'country_of_origin', hue_col='gender')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Education and language proficiency by gender\nviewCategoricalBreakdown(df, 'education_level', hue_col='gender')\nviewCategoricalBreakdown(df, 'language_proficiency', hue_col='gender')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Trauma rate by country of origin\ntrauma_by_country = (\n    df.groupby('country_of_origin')['reported_trauma']\n    .mean()\n    .sort_values(ascending=False)\n)\nplt.figure(figsize=(8, 4))\ntrauma_by_country.plot(kind='bar', color='#fc8d62', edgecolor='white')\nplt.ylabel('Proportion Reporting Trauma')\nplt.title('Trauma Rate by Country of Origin')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Examine Scoring Functions\n\nThe AI uses three scores: credibility, risk, and integration. Let's examine how these scores are distributed and whether they reflect fair or biased patterns."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Overall distribution of each score\nfor score in ['credibility_score', 'risk_score', 'integration_score']:\n    viewScoreDistribution(df, score)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Does language proficiency inflate credibility? (Intentional bias)\nviewScoreDistribution(df, 'credibility_score', hue_col='language_proficiency')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Does reported trauma deflate credibility? (Intentional bias)\nviewScoreDistribution(df, 'credibility_score', hue_col='reported_trauma')\n\nprint(\"Average credibility score by trauma status:\")\nprint(df.groupby('reported_trauma')['credibility_score'].mean().round(3))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Risk score: how many cases hit the 1.0 ceiling?\ncapped_cases = (df['risk_score'] == 1.0).sum()\nprint(f\"Cases where risk_score was capped at 1.0: {capped_cases}\")\nprint(f\"Max uncapped value: {df['risk_score_uncapped'].max():.3f}\")\n\nviewScoreDistribution(df, 'risk_score_uncapped')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Correlations between scores and key variables\nplotCorrelationHeatmap(df, ['credibility_score', 'risk_score',\n                             'integration_score', 'state_protection_score', 'age'])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Analyze AI & Final Decisions\n\nHow does the AI decide? Are certain groups approved at higher rates?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# AI decision vs final decision \u2014 did human review change things?\nviewDecisionDistribution(df, decision_col='AI_decision')\nviewDecisionDistribution(df, decision_col='final_decision')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Approval rate by country of origin\nviewApprovalRateBy(df, 'country_of_origin')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Approval rate by gender\nviewApprovalRateBy(df, 'gender')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Approval rate by education level and persecution type\nviewApprovalRateBy(df, 'education_level')\nviewApprovalRateBy(df, 'persecution_type')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Score distributions by final decision\nfor score in ['credibility_score', 'risk_score', 'integration_score']:\n    viewScoreByDecision(df, score)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cases denied despite nexus being established \u2014 legally suspicious outcomes\nnexus_denied = df[(df['nexus_established'] == True) & (df['final_decision'] == 'deny')]\nprint(f\"Cases denied despite nexus established: {len(nexus_denied)}\")\nprint(f\"That is {len(nexus_denied)/len(df):.1%} of all cases\")\nprint(\"\\nBreakdown by country:\")\nprint(nexus_denied['country_of_origin'].value_counts())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Human Oversight & Override Patterns\n\nOnly ~10% of cases are reviewed by a human. Does oversight meaningfully correct AI errors?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# How many cases were reviewed vs overridden?\nprint(f\"Cases reviewed by human:  {df['human_reviewed'].sum()} ({df['human_reviewed'].mean():.1%})\")\nprint(f\"Cases overridden:         {df['human_override'].sum()} ({df['human_override'].mean():.1%})\")\nprint(f\"Of reviewed cases, overridden: {df[df['human_reviewed']]['human_override'].mean():.1%}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Did overrides help the right people?\noverridden = df[df['human_override'] == True]\nprint(\"AI decision vs final decision among overridden cases:\")\nprint(overridden[['AI_decision', 'final_decision']].value_counts())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Processing time: oversight vs efficiency tradeoff\nviewScoreDistribution(df, 'processing_time_days', hue_col='human_reviewed')\nprint(\"\\nAverage processing time (days):\")\nprint(df.groupby('human_reviewed')['processing_time_days'].mean().round(1))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Processing time by final decision\nviewScoreByDecision(df, 'processing_time_days')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Appeals & Bias Audit"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Who appeals?\ndenied = df[df['final_decision'] == 'deny']\nprint(f\"Cases that appealed: {df['appealed'].sum()} ({denied['appealed'].mean():.1%} of denied cases)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Appeal outcomes\nappeal_counts = df[df['appealed'] == True]['appeal_outcome'].value_counts()\nplt.figure(figsize=(5, 4))\nplt.pie(appeal_counts, labels=appeal_counts.index, autopct='%1.1f%%',\n        colors=['#fc8d62', '#66c2a5'], startangle=90)\nplt.title('Appeal Outcomes')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Who gets overturned on appeal?\noverturned = df[df['appeal_outcome'] == 'overturned']\nprint(\"Country breakdown of overturned appeals:\")\nprint(overturned['country_of_origin'].value_counts())\nprint(\"\\nGender breakdown of overturned appeals:\")\nprint(overturned['gender'].value_counts())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Bias flag distribution\nviewCategoricalBreakdown(df, 'bias_flag', hue_col='final_decision')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# What characterizes severe bias flag cases?\nsevere = df[df['bias_flag'] == 'severe']\nprint(f\"Severe bias flag cases: {len(severe)}\")\nprint(f\"\\nTrauma rate in severe cases: {severe['reported_trauma'].mean():.1%}\")\nprint(f\"Trauma rate overall:         {df['reported_trauma'].mean():.1%}\")\nprint(f\"\\nAvg credibility in severe cases: {severe['credibility_score'].mean():.3f}\")\nprint(f\"Avg credibility overall:         {df['credibility_score'].mean():.3f}\")\nprint(f\"\\nNexus established in severe cases: {severe['nexus_established'].mean():.1%}\")\nprint(f\"Nexus established overall:         {df['nexus_established'].mean():.1%}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Build a Predictive Model (Extension)\n\nTrain a Random Forest to predict `final_decision` and examine which features the model considers most important. Compare this to what you found visually above."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "features = df.drop(columns=['id', 'final_decision', 'AI_decision',\n                              'human_reviewed', 'human_override',\n                              'appealed', 'appeal_outcome', 'bias_flag',\n                              'risk_score_uncapped'])\ntargets = (df['final_decision'] == 'approve').astype(int)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    features, targets, test_size=0.3, random_state=42\n)\n\nX_train_dum = pd.get_dummies(X_train, drop_first=True)\nX_test_dum  = pd.get_dummies(X_test, drop_first=True).reindex(\n    columns=X_train_dum.columns, fill_value=0\n)\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\nmodel.fit(X_train_dum, y_train)\n\ny_pred = model.predict(X_test_dum)\nprint(f\"Model accuracy: {accuracy_score(y_test, y_pred):.3f}\")\nprint(\"\\nConfusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "plotFeatureImportance(model, X_train_dum)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Discussion Questions\n\nUse your analysis above to reflect on the following:\n\n**1. Credibility Bias**\nCredibility scores are partly driven by language proficiency and education. Is this a fair proxy for truthfulness? What groups are most disadvantaged by this design?\n\n**2. Trauma Penalty**\nApplicants with reported trauma receive a lower credibility score. What does this mean for survivors of sexual violence or detention? Is there a way to correct for this?\n\n**3. Country-of-Origin Patterns**\nWhich countries have the lowest approval rates? Is the variation driven by risk scores, credibility, nexus rates \u2014 or something else?\n\n**4. Human Oversight**\nOnly ~10% of cases are reviewed by a human, and not all overrides help. Does human review meaningfully correct AI errors, or does it mostly rubber-stamp them?\n\n**5. Appeals as a Safety Net**\n40% of appealed cases are overturned. What does this suggest about the accuracy of the original AI decision? Who bears the cost of waiting for an appeal?\n\n**6. Feature Importance**\nLook at the Random Forest feature importance chart. Which features drive decisions most? Are any of them proxies for sensitive attributes like gender or country of origin?"
  }
 ]
}